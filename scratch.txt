================================================================================
CODE REVIEW & EXECUTION SIMULATION - API UPLOAD ENDPOINT WITH OVERRIDES
================================================================================
Date: November 13, 2025
Test Case: API file upload with extraction_method, llm_processing_mode, and ocr_engine overrides
Command: POST /api/v1/extract/upload
Request: multipart/form-data with file + parameter overrides

================================================================================
INITIAL STATE
================================================================================
ENVIRONMENT:
- Python 3.10.0
- FastAPI application running on http://localhost:8000
- Working directory: D:\dev\projects\msa_extractor
- Config values:
  - DB_PATH: storage/msa_extractor.db
  - UPLOADS_DIR: uploads/
  - EXTRACTION_METHOD: hybrid (default)
  - LLM_PROCESSING_MODE: text_llm (default)
  - OCR_ENGINE: tesseract (default)
  - API_ENABLE_AUTH: false (auth disabled for this test)
  - MAX_UPLOAD_SIZE_MB: 25

FILE SYSTEM:
- Input file: contract.pdf (client uploads via HTTP)
- Database: storage/msa_extractor.db (exists)
- Directories: uploads/, storage/ (exist)

HTTP REQUEST:
- Method: POST
- URL: /api/v1/extract/upload
- Content-Type: multipart/form-data
- Body:
  - file: contract.pdf (2.5 MB)
  - extraction_method: "ocr_images_only"
  - llm_processing_mode: "multimodal"
  - ocr_engine: "gcv"

================================================================================
EXECUTION FLOW - STEP BY STEP
================================================================================

STEP 1: FastAPI Request Routing
--------------------------------------------------------------------------------
Line: api/main.py - app routing
- FastAPI receives POST request to /api/v1/extract/upload
- Routes to api/routers/extract.py::upload_file()

STEP 2: Dependency Injection - API Key Verification
--------------------------------------------------------------------------------
Line: api/dependencies.py - verify_api_key()
- API_ENABLE_AUTH = False, so returns "anonymous"
- No authentication required

STEP 3: Dependency Injection - Database Connection
--------------------------------------------------------------------------------
Line: api/dependencies.py - get_db()
- Creates ExtractionDB instance
- Opens connection to storage/msa_extractor.db
- Yields db instance (will close in finally block)

OBJECT STATE:
- db: ExtractionDB instance
- db.conn: SQLite connection (active)

STEP 4: api/routers/extract.py - upload_file() Entry
--------------------------------------------------------------------------------
Line 89: Function signature
- background_tasks: FastAPI BackgroundTasks instance
- file: UploadFile object (contract.pdf, 2.5 MB)
- extraction_method: "ocr_images_only" (from form data)
- llm_processing_mode: "multimodal" (from form data)
- ocr_engine: "gcv" (from form data)
- db: ExtractionDB (from dependency)
- api_key: "anonymous" (from dependency)

STEP 5: File Validation - Filename Check
--------------------------------------------------------------------------------
Line 129-133: Validate filename exists
- file.filename = "contract.pdf"
- Not empty, continues

STEP 6: File Type Validation
--------------------------------------------------------------------------------
Line 135: validate_file_type(file.filename)
- Checks if extension is .pdf or .docx
- "contract.pdf" -> .pdf -> VALID
- Continues

STEP 7: Read File Content
--------------------------------------------------------------------------------
Line 138: content = await file.read()
- Reads entire file into memory
- content: bytes object (2.5 MB)
- file_size = len(content) = 2,621,440 bytes

STEP 8: File Size Validation
--------------------------------------------------------------------------------
Line 140: validate_file_size(file_size)
- file_size = 2,621,440 bytes = ~2.5 MB
- MAX_UPLOAD_SIZE_BYTES = 25 * 1024 * 1024 = 26,214,400 bytes
- 2,621,440 < 26,214,400 -> VALID
- Continues

STEP 9: Generate Job ID
--------------------------------------------------------------------------------
Line 143: job_id = str(uuid.uuid4())
- Generates UUID: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
- job_id = "a1b2c3d4-e5f6-7890-abcd-ef1234567890"

OBJECT STATE:
- job_id: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"

STEP 10: Determine File Extension
--------------------------------------------------------------------------------
Line 146: file_extension = Path(file.filename).suffix.lower()
- file_extension = ".pdf"

STEP 11: Construct Storage Path
--------------------------------------------------------------------------------
Line 147: file_path = UPLOADS_DIR / f"{job_id}{file_extension}"
- UPLOADS_DIR = Path("uploads/")
- file_path = Path("uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf")

STEP 12: Create Uploads Directory
--------------------------------------------------------------------------------
Line 148: UPLOADS_DIR.mkdir(parents=True, exist_ok=True)
- Creates uploads/ directory if needed (already exists)

STEP 13: Write File to Disk
--------------------------------------------------------------------------------
Line 151-152: Write file content
- Opens file_path in binary write mode
- Writes content bytes to disk
- File saved: uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf

OBJECT STATE:
- File on disk: uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf (2.5 MB)

STEP 14: Log File Upload
--------------------------------------------------------------------------------
Line 154: logger.info(...)
- Logs: "File uploaded: contract.pdf -> uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf (Job ID: a1b2c3d4-e5f6-7890-abcd-ef1234567890)"

STEP 15: Determine Extraction Method
--------------------------------------------------------------------------------
Line 158: extraction_method_used = extraction_method or EXTRACTION_METHOD
- extraction_method = "ocr_images_only" (provided)
- extraction_method_used = "ocr_images_only"

STEP 16: Determine LLM Processing Mode
--------------------------------------------------------------------------------
Line 159: llm_mode_used = llm_processing_mode or LLM_PROCESSING_MODE
- llm_processing_mode = "multimodal" (provided)
- llm_mode_used = "multimodal"

STEP 17: Determine OCR Engine
--------------------------------------------------------------------------------
Line 161-166: OCR engine logic
- ocr_engine = "gcv" (provided, not None)
- extraction_method_used = "ocr_images_only" (requires OCR)
- Since ocr_engine is provided, use it
- ocr_engine_used = "gcv"

OBJECT STATE:
- extraction_method_used: "ocr_images_only"
- llm_mode_used: "multimodal"
- ocr_engine_used: "gcv"

STEP 18: Create Job in Database
--------------------------------------------------------------------------------
Line 169-177: db.create_job(...)
- file_name="contract.pdf"
- pdf_storage_path="uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf"
- file_size=2621440
- extraction_method="ocr_images_only"
- llm_processing_mode="multimodal"
- ocr_engine="gcv"
- job_id="a1b2c3d4-e5f6-7890-abcd-ef1234567890"

STEP 19: storage/database.py - create_job() Execution
--------------------------------------------------------------------------------
Line 186: job_id parameter provided, uses it (doesn't generate new one)
Line 189-197: INSERT INTO extractions (...)
- Inserts job record with:
  - id: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
  - status: "pending"
  - extraction_method: "ocr_images_only"
  - llm_processing_mode: "multimodal"
  - ocr_engine: "gcv"
- Commits transaction

OBJECT STATE:
- Job in database: status="pending", all parameters stored

STEP 20: Schedule Background Task
--------------------------------------------------------------------------------
Line 180-187: background_tasks.add_task(...)
- Schedules process_extraction() to run after response
- Parameters:
  - job_id: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
  - file_path: Path("uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf")
  - extraction_method: "ocr_images_only"
  - llm_processing_mode: "multimodal"
  - ocr_engine: "gcv"

STEP 21: Get Job Timestamp
--------------------------------------------------------------------------------
Line 190: job = db.get_job(job_id)
- Queries database for job record
- Returns job dict with created_at timestamp

STEP 22: Build Response
--------------------------------------------------------------------------------
Line 194-202: UploadResponse(...)
- job_id: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
- status: "pending"
- file_name: "contract.pdf"
- file_size: 2621440
- created_at: "2025-11-13T10:30:00Z" (from database)
- status_url: "/api/v1/extract/status/a1b2c3d4-e5f6-7890-abcd-ef1234567890"
- result_url: "/api/v1/extract/result/a1b2c3d4-e5f6-7890-abcd-ef1234567890"

STEP 23: Return HTTP Response
--------------------------------------------------------------------------------
Line 202: return UploadResponse(...)
- FastAPI serializes to JSON
- HTTP 201 Created response sent to client
- Response body contains job_id and URLs

STEP 24: Dependency Cleanup - Database Connection
--------------------------------------------------------------------------------
Line: api/dependencies.py - get_db() finally block
- db.close() called
- Database connection closed

HTTP RESPONSE SENT:
- Status: 201 Created
- Body: {
    "job_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "status": "pending",
    "file_name": "contract.pdf",
    "file_size": 2621440,
    "created_at": "2025-11-13T10:30:00Z",
    "status_url": "/api/v1/extract/status/a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "result_url": "/api/v1/extract/result/a1b2c3d4-e5f6-7890-abcd-ef1234567890"
  }

================================================================================
BACKGROUND TASK EXECUTION - process_extraction()
================================================================================

STEP 25: Background Task Starts
--------------------------------------------------------------------------------
Line: api/services/extraction_service.py - process_extraction()
- FastAPI background task executor calls function
- Parameters received:
  - job_id: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
  - file_path: Path("uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf")
  - extraction_method: "ocr_images_only"
  - llm_processing_mode: "multimodal"
  - ocr_engine: "gcv"
  - db: None (will create new connection)

STEP 26: Create Database Connection
--------------------------------------------------------------------------------
Line 58-60: Create database if not provided
- db = None, so creates new ExtractionDB()
- db_created = True (flag for cleanup)
- Opens new SQLite connection

OBJECT STATE:
- db: ExtractionDB instance (new connection)
- db_created: True

STEP 27: Validate Configuration
--------------------------------------------------------------------------------
Line 64: validate_config()
- Checks GEMINI_API_KEY is set
- Validates all required config values
- Continues if valid

STEP 28: Update Job Status to Processing
--------------------------------------------------------------------------------
Line 67-71: db.update_job_status(...)
- Updates job status: "pending" -> "processing"
- Sets started_at: datetime.now()
- Commits transaction

STEP 29: Log Extraction Start
--------------------------------------------------------------------------------
Line 72: db.add_log_entry(...)
- Adds log entry: "Starting extraction"
- Level: INFO
- Module: api.services.extraction_service

STEP 30: Initialize Extraction Coordinator
--------------------------------------------------------------------------------
Line 75: coordinator = ExtractionCoordinator()
- Creates coordinator instance
- Initializes StrategyFactory
- Initializes GeminiClient
- Initializes logger

OBJECT STATE:
- coordinator: ExtractionCoordinator instance
- coordinator.strategy_factory: StrategyFactory
- coordinator.gemini_client: GeminiClient

STEP 31: Log Override Parameters
--------------------------------------------------------------------------------
Line 79-83: Log override usage
- extraction_method = "ocr_images_only" (not None)
- llm_processing_mode = "multimodal" (not None)
- ocr_engine = "gcv" (not None)
- Logs: "Using overrides: extraction_method=ocr_images_only, llm_processing_mode=multimodal, ocr_engine=gcv"

STEP 32: Call Extraction Coordinator
--------------------------------------------------------------------------------
Line 85-91: coordinator.extract_metadata(...)
- file_path: "uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf"
- strategy: None (deprecated)
- extraction_method: "ocr_images_only" (override)
- llm_processing_mode: "multimodal" (override)
- ocr_engine: "gcv" (override)

STEP 33: extractors/extraction_coordinator.py - extract_metadata() Entry
--------------------------------------------------------------------------------
Line 45-52: Function entry
- Receives override parameters
- extraction_method: "ocr_images_only"
- llm_processing_mode: "multimodal"
- ocr_engine: "gcv"

STEP 34: Determine Used Parameters
--------------------------------------------------------------------------------
Line 75-77: Use overrides or config defaults
- extraction_method_used = "ocr_images_only" (provided, not None)
- llm_mode_used = "multimodal" (provided, not None)
- ocr_engine_used = "gcv" (provided, not None)

OBJECT STATE:
- extraction_method_used: "ocr_images_only"
- llm_mode_used: "multimodal"
- ocr_engine_used: "gcv"

STEP 35: Extract Text from Document
--------------------------------------------------------------------------------
Line 80-85: _extract_text(...)
- file_path: "uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf"
- strategy: None
- extraction_method: "ocr_images_only"
- ocr_engine: "gcv"

STEP 36: _extract_text() - Determine File Type
--------------------------------------------------------------------------------
Line 116: file_ext = Path(file_path).suffix.lower()
- file_ext = ".pdf"
- Routes to _extract_from_pdf()

STEP 37: _extract_from_pdf() - Get Extractor
--------------------------------------------------------------------------------
Line 147: extractor = self.strategy_factory.get_extractor(file_path, strategy)
- strategy = None, uses config EXTRACTION_METHOD for extractor selection
- Returns appropriate PDF extractor (PDFExtractor or GeminiVisionExtractor)

STEP 38: _extract_from_pdf() - Extract Content
--------------------------------------------------------------------------------
Line 148: result = extractor.extract(file_path)
- Extracts text/images from PDF
- Returns ExtractedTextResult with:
  - raw_text: extracted text (may be empty for image-only PDFs)
  - metadata: dict with extraction info
  - preprocessed_images: list of image arrays (if any)

STEP 39: Determine Extraction Method Used
--------------------------------------------------------------------------------
Line 151-155: Use override or result metadata or config
- extraction_method = "ocr_images_only" (provided)
- result.metadata.get("extraction_method") = None (or from extractor)
- EXTRACTION_METHOD = "hybrid" (config default)
- extraction_method_used = "ocr_images_only" (override takes precedence)

STEP 40: Update Result Metadata
--------------------------------------------------------------------------------
Line 158: result.metadata["extraction_method"] = "ocr_images_only"
- Updates result metadata with override value

STEP 41: Check if OCR is Needed
--------------------------------------------------------------------------------
Line 161: preprocessed_images = result.metadata.get("preprocessed_images")
- preprocessed_images: list of image arrays (from PDF extraction)
- extraction_method_used = "ocr_images_only" (requires OCR)
- Condition: preprocessed_images exists AND extraction_method in ["ocr_all", "ocr_images_only"]
- Condition: True (images exist and method requires OCR)

STEP 42: Determine OCR Engine to Use
--------------------------------------------------------------------------------
Line 165: ocr_engine_used = ocr_engine or OCR_ENGINE
- ocr_engine = "gcv" (provided override)
- ocr_engine_used = "gcv"

OBJECT STATE:
- ocr_engine_used: "gcv" (override used, not config default)

STEP 43: Create OCR Handler with Override
--------------------------------------------------------------------------------
Line 168: ocr_handler = OCRHandler(ocr_engine=ocr_engine_used)
- Creates OCRHandler with ocr_engine="gcv"
- Uses Google Cloud Vision API (not Tesseract)

STEP 44: Run OCR on Images
--------------------------------------------------------------------------------
Line 169: ocr_texts = ocr_handler.extract_text_from_images(preprocessed_images)
- Processes all preprocessed images with GCV
- Returns list of extracted text strings
- Each image processed through Google Cloud Vision API

STEP 45: Combine OCR Text with Existing Text
--------------------------------------------------------------------------------
Line 172-175: Combine text based on extraction method
- extraction_method_used = "ocr_images_only"
- result.raw_text: existing text (may be empty)
- If "ocr_images_only" and raw_text exists:
  - Combine: result.raw_text += "\n\n" + "\n\n".join(ocr_texts)
- Else (ocr_images_only, no existing text):
  - Replace: result.raw_text = "\n\n".join(ocr_texts)
- In this case: result.raw_text = "\n\n".join(ocr_texts) (only OCR text)

STEP 46: Update Result Metadata with OCR Engine
--------------------------------------------------------------------------------
Line 177: result.metadata["ocr_engine"] = "gcv"
- Stores OCR engine used in metadata

STEP 47: Clear Preprocessed Images from Memory
--------------------------------------------------------------------------------
Line 180-182: Delete images from memory
- Removes "preprocessed_images" from result.metadata
- Frees memory (images no longer needed after OCR)

OBJECT STATE:
- result.raw_text: Combined OCR text from all images
- result.metadata["extraction_method"]: "ocr_images_only"
- result.metadata["ocr_engine"]: "gcv"
- result.metadata["preprocessed_images"]: deleted

STEP 48: Return Extraction Result
--------------------------------------------------------------------------------
Line 184: return result
- Returns ExtractedTextResult to extract_metadata()

STEP 49: Process with LLM
--------------------------------------------------------------------------------
Line 88-93: Determine LLM mode and process
- llm_mode_used = "multimodal" (override)
- extraction_result.metadata.get("llm_processing_mode") = None
- LLM_PROCESSING_MODE = "text_llm" (config default)
- llm_mode = "multimodal" (override takes precedence)

STEP 50: _process_with_llm() - Multimodal Mode
--------------------------------------------------------------------------------
Line 151-152: _process_with_llm(extraction_result, "multimodal")
- llm_mode = "multimodal"
- Routes to multimodal processing branch

STEP 51: Multimodal LLM Processing
--------------------------------------------------------------------------------
Line: _process_with_llm() multimodal branch
- Sends both text and images to Gemini Vision API
- Uses vision model for understanding
- Returns structured metadata dict

OBJECT STATE:
- metadata: dict with extracted MSA metadata
- All fields populated from LLM processing

STEP 52: Return Metadata
--------------------------------------------------------------------------------
Line 95: return metadata
- Returns metadata dict to process_extraction()

STEP 53: Validate Schema
--------------------------------------------------------------------------------
Line 94-95: validator.validate(metadata)
- Validates metadata against JSON schema
- is_valid: True (or False with errors)
- error: None (or error message)

STEP 54: Handle Schema Validation Issues
--------------------------------------------------------------------------------
Line 97-106: If validation fails
- If is_valid = False:
  - Logs warning
  - Adds log entry to database
  - Normalizes metadata to fix structure
- In this case: is_valid = True, continues

STEP 55: Store Results in Database
--------------------------------------------------------------------------------
Line 109-116: db.complete_job(...)
- job_id: "a1b2c3d4-e5f6-7890-abcd-ef1234567890"
- result_json_dict: metadata dict
- pdf_storage_path: "uploads/a1b2c3d4-e5f6-7890-abcd-ef1234567890.pdf"
- pdf_storage_type: "local"
- result_json_path: None (database storage)
- log_path: None (database storage)

STEP 56: storage/database.py - complete_job() Execution
--------------------------------------------------------------------------------
Line 309: result_json_str = json.dumps(metadata, ensure_ascii=False)
- Serializes metadata to JSON string

Line 312-329: UPDATE extractions SET ...
- status: "processing" -> "completed"
- completed_at: CURRENT_TIMESTAMP
- result_json: JSON string (stored in database)
- pdf_storage_path: updated
- Commits transaction

OBJECT STATE:
- Job status: "completed"
- result_json: Stored in database as TEXT
- completed_at: Timestamp set

STEP 57: Log Completion
--------------------------------------------------------------------------------
Line 118-123: db.add_log_entry(...)
- Adds log entry: "Extraction completed successfully"
- Level: INFO
- Module: api.services.extraction_service

STEP 58: Log to Logger
--------------------------------------------------------------------------------
Line 124: logger.info(...)
- Logs: "Extraction completed for job a1b2c3d4-e5f6-7890-abcd-ef1234567890"

STEP 59: Exception Handling - Success Path
--------------------------------------------------------------------------------
Line 125-163: Exception handlers
- No exceptions raised, skips all except blocks
- Continues to finally block

STEP 60: Cleanup Database Connection
--------------------------------------------------------------------------------
Line 164-166: finally block
- db_created = True
- db is not None
- db.close() called
- Database connection closed

BACKGROUND TASK COMPLETED:
- Job status: "completed"
- Results stored in database
- All overrides used correctly:
  - extraction_method: "ocr_images_only" ✓
  - llm_processing_mode: "multimodal" ✓
  - ocr_engine: "gcv" ✓

================================================================================
ISSUES IDENTIFIED
================================================================================

P0 - CRITICAL BUGS:
None identified in this flow.

P1 - HIGH PRIORITY ISSUES:
1. **Background task database connection management**
   - Location: api/services/extraction_service.py line 58-60
   - Issue: Creates new database connection for each background task
   - Impact: Multiple connections if many concurrent tasks
   - Fix: Consider connection pooling or reuse singleton connection
   - Note: Current implementation is acceptable for low concurrency

2. **File cleanup on job failure**
   - Location: api/services/extraction_service.py exception handlers
   - Issue: If job fails, PDF file remains in uploads/ directory
   - Impact: Disk space waste over time
   - Fix: Add file cleanup in error handlers (delete file_path on failure)

3. **Memory usage with large files**
   - Location: api/routers/extract.py line 138
   - Issue: Reads entire file into memory (await file.read())
   - Impact: High memory usage for large files (up to 25MB)
   - Fix: Consider streaming file write for very large files
   - Note: Current approach is acceptable for 25MB limit

P2 - MEDIUM PRIORITY ISSUES:
4. **OCR engine override validation**
   - Location: api/routers/extract.py line 100-103
   - Issue: No validation that ocr_engine is "tesseract" or "gcv"
   - Impact: Invalid values passed to OCRHandler may cause errors
   - Fix: Add validation against allowed values

5. **Extraction method override validation**
   - Location: api/routers/extract.py line 92-95
   - Issue: No validation that extraction_method is in allowed list
   - Impact: Invalid values may cause extraction to fail
   - Fix: Add validation against allowed extraction methods

6. **LLM processing mode override validation**
   - Location: api/routers/extract.py line 96-99
   - Issue: No validation that llm_processing_mode is in allowed list
   - Impact: Invalid values may cause LLM processing to fail
   - Fix: Add validation against allowed LLM modes

7. **Strategy parameter still passed as None**
   - Location: api/services/extraction_service.py line 87
   - Issue: strategy=None passed to extract_metadata() (deprecated parameter)
   - Impact: Minor, but could be removed in future cleanup
   - Fix: Remove strategy parameter once fully deprecated

P3 - LOW PRIORITY / OPTIMIZATIONS:
8. **Redundant parameter determination**
   - Location: api/routers/extract.py lines 158-166
   - Issue: Determines extraction_method_used, llm_mode_used, ocr_engine_used
   - Then passes same values to background task
   - Impact: Minor code duplication
   - Fix: Could pass original parameters and determine in background task

9. **Database query for created_at**
   - Location: api/routers/extract.py line 190
   - Issue: Queries database just to get created_at timestamp
   - Impact: Extra database query
   - Fix: Could use datetime.now() directly (job was just created)

10. **Logging override parameters twice**
    - Location: api/services/extraction_service.py lines 79-83
    - Issue: Logs override parameters, but they're also in database job record
    - Impact: Redundant logging
    - Fix: Could remove logging or make it more concise

================================================================================
PARAMETER OVERRIDE FLOW VERIFICATION
================================================================================

✓ extraction_method override:
  - Received in API: "ocr_images_only"
  - Stored in database: "ocr_images_only"
  - Passed to background task: "ocr_images_only"
  - Used in coordinator: "ocr_images_only"
  - Used in OCR decision: "ocr_images_only" (triggers OCR)
  - RESULT: Override correctly used throughout

✓ llm_processing_mode override:
  - Received in API: "multimodal"
  - Stored in database: "multimodal"
  - Passed to background task: "multimodal"
  - Used in coordinator: "multimodal"
  - Used in LLM processing: "multimodal" (multimodal branch)
  - RESULT: Override correctly used throughout

✓ ocr_engine override:
  - Received in API: "gcv"
  - Stored in database: "gcv"
  - Passed to background task: "gcv"
  - Used in coordinator: "gcv"
  - Used in OCRHandler: "gcv" (Google Cloud Vision, not Tesseract)
  - RESULT: Override correctly used throughout

ALL OVERRIDES WORKING CORRECTLY ✓

================================================================================
MEMORY USAGE
================================================================================

- File upload: 2.5 MB in memory (content bytes)
- Database connection: ~1-2 KB per connection
- Job metadata: ~200-500 bytes per job
- Result JSON: ~1-5 KB per result
- Preprocessed images: ~10-50 MB (temporary, cleared after OCR)
- OCR text: ~100-500 KB (depends on document size)
- LLM response: ~1-5 KB (metadata dict)

Peak memory during extraction: ~60-70 MB (including images)
After OCR: ~15-20 MB (images cleared)

================================================================================
END OF SIMULATION
================================================================================

